#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
scripts/03_upload_to_bigquery.py

–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ BigQuery
"""

import os
import pandas as pd
from google.cloud import bigquery
from google.cloud.exceptions import NotFound
from dotenv import load_dotenv
from pathlib import Path
from datetime import datetime
import time

# ============================================================================
# –ù–ê–°–¢–†–û–ô–ö–ò
# ============================================================================

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ credentials
CREDENTIALS_PATH = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')
PROJECT_ID = os.getenv('GCP_PROJECT_ID')
DATASET_ID = os.getenv('BIGQUERY_DATASET', 'tech_survey_data')

# –ü—É—Ç—å –∫ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º
DATA_DIR = 'data/processed'

# –°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
FILES_TO_UPLOAD = [
    'demographics.csv',
    'language_haveworked.csv',
    'language_wanttowork.csv',
    'database_haveworked.csv',
    'database_wanttowork.csv',
    'platform_haveworked.csv',
    'platform_wanttowork.csv',
    'webframe_haveworked.csv',
    'webframe_wanttowork.csv'
]

# –°—Ö–µ–º—ã —Ç–∞–±–ª–∏—Ü
TABLE_SCHEMAS = {
    'demographics': [
        bigquery.SchemaField("ResponseId", "INTEGER", mode="REQUIRED"),
        bigquery.SchemaField("Country", "STRING", mode="NULLABLE"),
        bigquery.SchemaField("Age", "STRING", mode="NULLABLE"),
        bigquery.SchemaField("EdLevel", "STRING", mode="NULLABLE"),
        bigquery.SchemaField("YearsCode", "STRING", mode="NULLABLE"),
        bigquery.SchemaField("YearsCodePro", "STRING", mode="NULLABLE"),
        bigquery.SchemaField("Employment", "STRING", mode="NULLABLE"),
        bigquery.SchemaField("RemoteWork", "STRING", mode="NULLABLE"),
        bigquery.SchemaField("DevType", "STRING", mode="NULLABLE"),
        bigquery.SchemaField("OrgSize", "STRING", mode="NULLABLE"),
        bigquery.SchemaField("Country_IsValid", "BOOLEAN", mode="NULLABLE"),
        bigquery.SchemaField("Age_IsValid", "BOOLEAN", mode="NULLABLE"),
        bigquery.SchemaField("EdLevel_IsValid", "BOOLEAN", mode="NULLABLE"),
        bigquery.SchemaField("YearsCode_IsValid", "BOOLEAN", mode="NULLABLE"),
        bigquery.SchemaField("YearsCodePro_IsValid", "BOOLEAN", mode="NULLABLE"),
        bigquery.SchemaField("Employment_IsValid", "BOOLEAN", mode="NULLABLE"),
        bigquery.SchemaField("RemoteWork_IsValid", "BOOLEAN", mode="NULLABLE"),
        bigquery.SchemaField("DevType_IsValid", "BOOLEAN", mode="NULLABLE"),
        bigquery.SchemaField("OrgSize_IsValid", "BOOLEAN", mode="NULLABLE"),
        bigquery.SchemaField("CreatedAt", "TIMESTAMP", mode="NULLABLE"),
    ],
    'technology': [
        bigquery.SchemaField("ResponseId", "INTEGER", mode="REQUIRED"),
        bigquery.SchemaField("Technology", "STRING", mode="REQUIRED"),
    ]
}

# ============================================================================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ============================================================================

def print_header(text):
    """–ü–µ—á–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
    print("\n" + "="*70)
    print(text)
    print("="*70)

def print_subheader(text):
    """–ü–µ—á–∞—Ç—å –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∞"""
    print("\n" + "‚îÄ"*70)
    print(text)
    print("‚îÄ"*70)

def check_credentials():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ credentials"""
    print_header("üîê –ü–†–û–í–ï–†–ö–ê CREDENTIALS")
    
    if not os.path.exists(CREDENTIALS_PATH):
        raise FileNotFoundError(f"Credentials —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {CREDENTIALS_PATH}")
    
    print(f"‚úì Credentials —Ñ–∞–π–ª: {CREDENTIALS_PATH}")
    
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = CREDENTIALS_PATH
    print(f"‚úì –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")

def init_bigquery_client():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è BigQuery –∫–ª–∏–µ–Ω—Ç–∞"""
    print_header("üîå –ü–û–î–ö–õ–Æ–ß–ï–ù–ò–ï –ö BIGQUERY")
    
    print(f"Project ID: {PROJECT_ID}")
    print(f"Dataset ID: {DATASET_ID}")
    
    try:
        client = bigquery.Client(project=PROJECT_ID)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        query = "SELECT 1 as test"
        result = client.query(query).result()
        
        print("‚úì –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ")
        return client
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")
        raise

def check_dataset_exists(client, dataset_id):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è dataset"""
    print_subheader(f"üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ dataset: {dataset_id}")
    
    try:
        dataset_ref = f"{PROJECT_ID}.{dataset_id}"
        dataset = client.get_dataset(dataset_ref)
        print(f"‚úì Dataset —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {dataset_ref}")
        print(f"  Location: {dataset.location}")
        print(f"  Created: {dataset.created}")
        return True
        
    except NotFound:
        print(f"‚ùå Dataset –Ω–µ –Ω–∞–π–¥–µ–Ω: {dataset_ref}")
        print("\n–°–æ–∑–¥–∞–π—Ç–µ dataset –≤—Ä—É—á–Ω—É—é:")
        print("1. –û—Ç–∫—Ä–æ–π—Ç–µ https://console.cloud.google.com/bigquery")
        print(f"2. –°–æ–∑–¥–∞–π—Ç–µ dataset —Å –∏–º–µ–Ω–µ–º: {dataset_id}")
        print("3. Location: US (–∏–ª–∏ EU)")
        return False

def get_table_schema(table_name):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ö–µ–º—ã –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã"""
    if table_name == 'demographics':
        return TABLE_SCHEMAS['demographics']
    else:
        return TABLE_SCHEMAS['technology']

def create_or_replace_table(client, dataset_id, table_name, schema):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏–ª–∏ –∑–∞–º–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü—ã"""
    table_id = f"{PROJECT_ID}.{dataset_id}.{table_name}"
    
    # –£–¥–∞–ª—è–µ–º —Ç–∞–±–ª–∏—Ü—É –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    try:
        client.delete_table(table_id)
        print(f"  ‚ö†Ô∏è  –°—É—â–µ—Å—Ç–≤—É—é—â–∞—è —Ç–∞–±–ª–∏—Ü–∞ —É–¥–∞–ª–µ–Ω–∞")
    except NotFound:
        pass
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É
    table = bigquery.Table(table_id, schema=schema)
    table = client.create_table(table)
    print(f"  ‚úì –¢–∞–±–ª–∏—Ü–∞ —Å–æ–∑–¥–∞–Ω–∞: {table_name}")
    
    return table

def upload_csv_to_bigquery(client, dataset_id, table_name, csv_path):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ CSV —Ñ–∞–π–ª–∞ –≤ BigQuery —Ç–∞–±–ª–∏—Ü—É
    """
    print_subheader(f"üì§ –ó–∞–≥—Ä—É–∑–∫–∞: {table_name}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
    if not os.path.exists(csv_path):
        print(f"  ‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {csv_path}")
        return False
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ
    file_size = os.path.getsize(csv_path) / 1024  # KB
    df = pd.read_csv(csv_path)
    print(f"  –§–∞–π–ª: {os.path.basename(csv_path)}")
    print(f"  –†–∞–∑–º–µ—Ä: {file_size:.1f} KB")
    print(f"  –°—Ç—Ä–æ–∫: {len(df):,}")
    print(f"  –°—Ç–æ–ª–±—Ü–æ–≤: {len(df.columns)}")
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ö–µ–º—ã
    schema = get_table_schema(table_name)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
    table = create_or_replace_table(client, dataset_id, table_name, schema)
    table_id = f"{PROJECT_ID}.{dataset_id}.{table_name}"
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ job –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
    job_config = bigquery.LoadJobConfig(
        source_format=bigquery.SourceFormat.CSV,
        skip_leading_rows=1,  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        autodetect=False,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —è–≤–Ω—É—é —Å—Ö–µ–º—É
        schema=schema,
        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,  # –ü–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º
        allow_quoted_newlines=True,  # –†–∞–∑—Ä–µ—à–∞–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –≤ –∫–∞–≤—ã—á–∫–∞—Ö
        max_bad_records=10  # –ú–∞–∫—Å–∏–º—É–º –ø–ª–æ—Ö–∏—Ö —Å—Ç—Ä–æ–∫
    )
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print(f"  üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ BigQuery...")
    start_time = time.time()
    
    try:
        with open(csv_path, "rb") as source_file:
            job = client.load_table_from_file(
                source_file,
                table_id,
                job_config=job_config
            )
        
        # –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è job
        job.result()
        
        elapsed_time = time.time() - start_time
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        table = client.get_table(table_id)
        
        print(f"  ‚úì –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed_time:.1f} —Å–µ–∫")
        print(f"  ‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫: {table.num_rows:,}")
        
        if job.errors:
            print(f"  ‚ö†Ô∏è  –û—à–∏–±–æ–∫ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {len(job.errors)}")
            for error in job.errors[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –æ—à–∏–±–æ–∫
                print(f"    - {error}")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        
        if hasattr(e, 'errors') and e.errors:
            print(f"\n  –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–æ–∫:")
            for error in e.errors[:5]:
                print(f"    {error}")
        
        return False

def verify_uploaded_data(client, dataset_id, table_name, expected_rows):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    table_id = f"{PROJECT_ID}.{dataset_id}.{table_name}"
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–∞–±–ª–∏—Ü–µ
        table = client.get_table(table_id)
        actual_rows = table.num_rows
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
        if actual_rows == expected_rows:
            print(f"    ‚úì –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ —Å–æ–≤–ø–∞–¥–∞–µ—Ç: {actual_rows:,}")
        else:
            print(f"    ‚ö†Ô∏è  –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç—Ä–æ–∫: {actual_rows:,} (–æ–∂–∏–¥–∞–ª–æ—Å—å {expected_rows:,})")
        
        # –ü–æ–ª—É—á–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        query = f"""
            SELECT *
            FROM `{table_id}`
            LIMIT 3
        """
        
        result = client.query(query).result()
        df_sample = result.to_dataframe()
        
        print(f"    ‚úì –î–∞–Ω–Ω—ã–µ –¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤")
        print(f"\n    –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö (–ø–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–æ–∫–∏):")
        print(df_sample.to_string(index=False))
        
        return True
        
    except Exception as e:
        print(f"    ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏: {e}")
        return False

def create_summary_report(results):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
    print_header("üìä –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
    
    successful = [r for r in results if r['success']]
    failed = [r for r in results if not r['success']]
    
    print(f"\n–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(successful)}/{len(results)}")
    
    if successful:
        print("\n‚úì –£—Å–ø–µ—à–Ω—ã–µ –∑–∞–≥—Ä—É–∑–∫–∏:")
        for result in successful:
            print(f"  ‚Ä¢ {result['table_name']}: {result['rows']:,} —Å—Ç—Ä–æ–∫")
    
    if failed:
        print("\n‚ùå –ù–µ—É–¥–∞—á–Ω—ã–µ –∑–∞–≥—Ä—É–∑–∫–∏:")
        for result in failed:
            print(f"  ‚Ä¢ {result['table_name']}: {result['error']}")
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_rows = sum(r['rows'] for r in successful)
    print(f"\nüìä –í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫: {total_rows:,}")
    
    return len(failed) == 0

# ============================================================================
# –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø
# ============================================================================

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print("\n" + "="*70)
    print("üöÄ –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –í BIGQUERY")
    print("="*70)
    print(f"–í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    results = []
    
    try:
        # ===== –®–ê–ì 1: –ü–†–û–í–ï–†–ö–ê CREDENTIALS =====
        check_credentials()
        
        # ===== –®–ê–ì 2: –ü–û–î–ö–õ–Æ–ß–ï–ù–ò–ï –ö BIGQUERY =====
        client = init_bigquery_client()
        
        # ===== –®–ê–ì 3: –ü–†–û–í–ï–†–ö–ê DATASET =====
        if not check_dataset_exists(client, DATASET_ID):
            print("\n‚ùå –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–π—Ç–µ dataset!")
            return 1
        
        # ===== –®–ê–ì 4: –ó–ê–ì–†–£–ó–ö–ê –§–ê–ô–õ–û–í =====
        print_header("üì§ –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•")
        
        for filename in FILES_TO_UPLOAD:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è —Ç–∞–±–ª–∏—Ü—ã (–±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è .csv)
            table_name = filename.replace('.csv', '')
            csv_path = os.path.join(DATA_DIR, filename)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª
            success = upload_csv_to_bigquery(client, DATASET_ID, table_name, csv_path)
            
            # –ï—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞ - –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ
            if success:
                df = pd.read_csv(csv_path)
                expected_rows = len(df)
                
                print(f"\n  üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
                verify_uploaded_data(client, DATASET_ID, table_name, expected_rows)
                
                results.append({
                    'table_name': table_name,
                    'success': True,
                    'rows': expected_rows,
                    'error': None
                })
            else:
                results.append({
                    'table_name': table_name,
                    'success': False,
                    'rows': 0,
                    'error': 'Upload failed'
                })
        
        # ===== –®–ê–ì 5: –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ =====
        all_success = create_summary_report(results)
        
        # ===== –ó–ê–í–ï–†–®–ï–ù–ò–ï =====
        if all_success:
            print_header("‚úÖ –í–°–ï –î–ê–ù–ù–´–ï –ó–ê–ì–†–£–ñ–ï–ù–´ –£–°–ü–ï–®–ù–û!")
            print(f"\nüìä Dataset: {PROJECT_ID}.{DATASET_ID}")
            print(f"üåê BigQuery Console: https://console.cloud.google.com/bigquery")
            print(f"‚è±Ô∏è  –í—Ä–µ–º—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            
            print("\n" + "="*70)
            print("üìù –°–õ–ï–î–£–Æ–©–ò–ô –®–ê–ì:")
            print("   –°–æ–∑–¥–∞–Ω–∏–µ SQL Views –¥–ª—è –¥–∞—à–±–æ—Ä–¥–∞")
            print("="*70)
            
            return 0
        else:
            print_header("‚ö†Ô∏è  –ó–ê–ì–†–£–ó–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê –° –û–®–ò–ë–ö–ê–ú–ò")
            return 1
        
    except Exception as e:
        print_header("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê!")
        print(f"\n{type(e).__name__}: {e}")
        
        import traceback
        print("\n–ü–æ–ª–Ω—ã–π traceback:")
        print(traceback.format_exc())
        
        return 1

# ============================================================================
# –¢–û–ß–ö–ê –í–•–û–î–ê
# ============================================================================

if __name__ == "__main__":
    exit(main())