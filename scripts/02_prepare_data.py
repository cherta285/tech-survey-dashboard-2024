#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
scripts/02_prepare_data.py

–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –æ–ø—Ä–æ—Å–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ BigQuery.
–°–æ–∑–¥–∞–µ—Ç:
1. demographics.csv - –¥–µ–º–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
2. 8 unpivot —Ç–∞–±–ª–∏—Ü –¥–ª—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π (Language, Database, Platform, Webframe)
"""

import pandas as pd
import numpy as np
from pathlib import Path
import os
from datetime import datetime

# ============================================================================
# –ö–û–ù–°–¢–ê–ù–¢–´ –ò –ù–ê–°–¢–†–û–ô–ö–ò
# ============================================================================

INPUT_FILE = 'data/raw/survey_results.csv'
OUTPUT_DIR = 'data/processed'

# –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Å—Ç–æ–ª–±—Ü—ã (–∏–∑ –≤–∞—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞)
TECH_COLUMNS_MAP = {
    'LanguageHaveWorkedWith': ('language', 'haveworked'),
    'LanguageWantToWorkWith': ('language', 'wanttowork'),
    'DatabaseHaveWorkedWith': ('database', 'haveworked'),
    'DatabaseWantToWorkWith': ('database', 'wanttowork'),
    'PlatformHaveWorkedWith': ('platform', 'haveworked'),
    'PlatformWantToWorkWith': ('platform', 'wanttowork'),
    'WebframeHaveWorkedWith': ('webframe', 'haveworked'),
    'WebframeWantToWorkWith': ('webframe', 'wanttowork')
}

# –î–µ–º–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ —Å—Ç–æ–ª–±—Ü—ã (–∫–ª—é—á–µ–≤—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞)
DEMO_COLUMNS = [
    'ResponseId',
    'Country',
    'Age',
    'EdLevel',
    'YearsCode',
    'YearsCodePro',
    'Employment',
    'RemoteWork',
    'DevType',
    'OrgSize'
]

# ============================================================================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ============================================================================

def print_header(text):
    """–ü–µ—á–∞—Ç—å –∫—Ä–∞—Å–∏–≤–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
    print("\n" + "="*70)
    print(text)
    print("="*70)

def print_subheader(text):
    """–ü–µ—á–∞—Ç—å –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∞"""
    print("\n" + "‚îÄ"*70)
    print(text)
    print("‚îÄ"*70)

def safe_strip(value):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤"""
    if pd.isna(value):
        return None
    return str(value).strip()

# ============================================================================
# –û–°–ù–û–í–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ============================================================================

def load_data(filepath):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    print_header("üìÇ –ó–ê–ì–†–£–ó–ö–ê –ò–°–•–û–î–ù–´–• –î–ê–ù–ù–´–•")
    
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"–§–∞–π–ª '{filepath}' –Ω–µ –Ω–∞–π–¥–µ–Ω!")
    
    print(f"–§–∞–π–ª: {filepath}")
    df = pd.read_csv(filepath, low_memory=False)
    print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫: {len(df):,}")
    print(f"‚úì –°—Ç–æ–ª–±—Ü–æ–≤: {len(df.columns):,}")
    
    return df

def create_demographics_table(df):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Å –¥–µ–º–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    """
    print_header("üë• –°–û–ó–î–ê–ù–ò–ï –¢–ê–ë–õ–ò–¶–´ DEMOGRAPHICS")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≤—Å–µ—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
    available_columns = [col for col in DEMO_COLUMNS if col in df.columns]
    missing_columns = [col for col in DEMO_COLUMNS if col not in df.columns]
    
    print(f"\n‚úì –î–æ—Å—Ç—É–ø–Ω–æ —Å—Ç–æ–ª–±—Ü–æ–≤: {len(available_columns)}/{len(DEMO_COLUMNS)}")
    if missing_columns:
        print(f"‚ö†Ô∏è  –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–æ–ª–±—Ü—ã: {', '.join(missing_columns)}")
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é —Å –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ —Å—Ç–æ–ª–±—Ü–∞–º–∏
    demo_df = df[available_columns].copy()
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    print("\nüîß –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...")
    
    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ (–∫—Ä–æ–º–µ ResponseId) —Å–æ–∑–¥–∞–µ–º —Ñ–ª–∞–≥ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏
    for col in available_columns:
        if col == 'ResponseId':
            continue
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–ª–∞–≥ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏
        is_valid_col = f"{col}_IsValid"
        demo_df[is_valid_col] = demo_df[col].notna() & (demo_df[col].astype(str).str.strip() != '')
        
        # –ó–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –Ω–∞ "Not Specified"
        demo_df[col] = demo_df[col].fillna('Not Specified')
        demo_df[col] = demo_df[col].replace('', 'Not Specified')
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        valid_count = demo_df[is_valid_col].sum()
        valid_percent = (valid_count / len(demo_df) * 100)
        print(f"  ‚Ä¢ {col}: {valid_count:,}/{len(demo_df):,} –≤–∞–ª–∏–¥–Ω—ã—Ö ({valid_percent:.1f}%)")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    demo_df['CreatedAt'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    print(f"\n‚úì –ò—Ç–æ–≥–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞: {len(demo_df):,} —Å—Ç—Ä–æ–∫ √ó {len(demo_df.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤")
    
    return demo_df

def create_technology_unpivot_table(df, source_column, tech_type, status):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ unpivot —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
    
    Args:
        df: –∏—Å—Ö–æ–¥–Ω—ã–π DataFrame
        source_column: –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–∞ —Å —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'LanguageHaveWorkedWith')
        tech_type: —Ç–∏–ø —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'language')
        status: —Å—Ç–∞—Ç—É—Å (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'haveworked')
    
    Returns:
        DataFrame —Å —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–º–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º–∏
    """
    print_subheader(f"üî® –û–±—Ä–∞–±–æ—Ç–∫–∞: {source_column}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Å—Ç–æ–ª–±—Ü–∞
    if source_column not in df.columns:
        print(f"‚ö†Ô∏è  –°—Ç–æ–ª–±–µ—Ü '{source_column}' –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        return None
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    total_rows = len(df)
    null_count = df[source_column].isna().sum()
    valid_count = total_rows - null_count
    
    print(f"  –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {total_rows:,}")
    print(f"  –í–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {valid_count:,} ({valid_count/total_rows*100:.1f}%)")
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –≤–∞–ª–∏–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    valid_data = df[df[source_column].notna()].copy()
    valid_data = valid_data[valid_data[source_column].astype(str).str.strip() != '']
    
    if len(valid_data) == 0:
        print(f"  ‚ö†Ô∏è  –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return None
    
    # –°–æ–∑–¥–∞–µ–º unpivot —Ç–∞–±–ª–∏—Ü—É
    unpivot_records = []
    
    for idx, row in valid_data.iterrows():
        response_id = row['ResponseId']
        tech_string = str(row[source_column])
        
        # –†–∞–∑–¥–µ–ª—è–µ–º —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –ø–æ ";"
        technologies = [tech.strip() for tech in tech_string.split(';') if tech.strip()]
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
        for tech in technologies:
            unpivot_records.append({
                'ResponseId': response_id,
                'Technology': tech
            })
    
    # –°–æ–∑–¥–∞–µ–º DataFrame
    unpivot_df = pd.DataFrame(unpivot_records)
    
    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (–µ—Å–ª–∏ —Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç —É–∫–∞–∑–∞–ª –æ–¥–Ω—É —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—é –¥–≤–∞–∂–¥—ã)
    before_dedup = len(unpivot_df)
    unpivot_df = unpivot_df.drop_duplicates(subset=['ResponseId', 'Technology'])
    after_dedup = len(unpivot_df)
    
    if before_dedup > after_dedup:
        print(f"  ‚ö†Ô∏è  –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {before_dedup - after_dedup:,}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    unique_respondents = unpivot_df['ResponseId'].nunique()
    unique_technologies = unpivot_df['Technology'].nunique()
    avg_tech_per_respondent = len(unpivot_df) / unique_respondents
    
    print(f"  ‚úì –°–æ–∑–¥–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(unpivot_df):,}")
    print(f"  ‚úì –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç–æ–≤: {unique_respondents:,}")
    print(f"  ‚úì –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π: {unique_technologies:,}")
    print(f"  ‚úì –°—Ä–µ–¥–Ω–µ–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –Ω–∞ —Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç–∞: {avg_tech_per_respondent:.1f}")
    
    # –¢–æ–ø-5 —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    top_5 = unpivot_df['Technology'].value_counts().head(5)
    print(f"\n  –¢–æ–ø-5 —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π:")
    for tech, count in top_5.items():
        print(f"    {count:>5,} - {tech}")
    
    return unpivot_df

def save_table(df, filename, output_dir):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –≤ CSV"""
    if df is None or len(df) == 0:
        print(f"  ‚ö†Ô∏è  –¢–∞–±–ª–∏—Ü–∞ –ø—É—Å—Ç–∞—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {filename}")
        return None
    
    filepath = os.path.join(output_dir, filename)
    df.to_csv(filepath, index=False, encoding='utf-8')
    
    # –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
    file_size = os.path.getsize(filepath) / 1024  # KB
    
    print(f"  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename}")
    print(f"    –°—Ç—Ä–æ–∫: {len(df):,}")
    print(f"    –°—Ç–æ–ª–±—Ü–æ–≤: {len(df.columns)}")
    print(f"    –†–∞–∑–º–µ—Ä: {file_size:.1f} KB")
    
    return filepath

def validate_data_integrity(df_original, created_files):
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    print_header("üîç –í–ê–õ–ò–î–ê–¶–ò–Ø –¶–ï–õ–û–°–¢–ù–û–°–¢–ò –î–ê–ù–ù–´–•")
    
    total_respondents = len(df_original)
    print(f"\n–ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç–æ–≤: {total_respondents:,}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ demographics
    demo_file = os.path.join(OUTPUT_DIR, 'demographics.csv')
    if os.path.exists(demo_file):
        demo_df = pd.read_csv(demo_file)
        demo_count = len(demo_df)
        
        if demo_count == total_respondents:
            print(f"‚úì demographics.csv: {demo_count:,} —Å—Ç—Ä–æ–∫ (—Å–æ–≤–ø–∞–¥–∞–µ—Ç)")
        else:
            print(f"‚ö†Ô∏è  demographics.csv: {demo_count:,} —Å—Ç—Ä–æ–∫ (–æ–∂–∏–¥–∞–ª–æ—Å—å {total_respondents:,})")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ç–∞–±–ª–∏—Ü
    print("\n–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ç–∞–±–ª–∏—Ü:")
    for tech_file in created_files:
        if 'demographics' in tech_file:
            continue
        
        tech_df = pd.read_csv(tech_file)
        unique_respondents = tech_df['ResponseId'].nunique()
        total_records = len(tech_df)
        
        filename = os.path.basename(tech_file)
        print(f"\n  {filename}:")
        print(f"    –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {total_records:,}")
        print(f"    –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç–æ–≤: {unique_respondents:,}")
        print(f"    –°—Ä–µ–¥–Ω–µ–µ –Ω–∞ —Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç–∞: {total_records/unique_respondents:.1f}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –∏—Å—Ö–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ
        original_ids = set(df_original['ResponseId'])
        tech_ids = set(tech_df['ResponseId'])
        missing_ids = tech_ids - original_ids
        
        if missing_ids:
            print(f"    ‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {len(missing_ids)} ID –Ω–µ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã!")
        else:
            print(f"    ‚úì –í—Å–µ ResponseId –≤–∞–ª–∏–¥–Ω—ã")

def create_summary_report(created_files):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
    print_header("üìÑ –°–û–ó–î–ê–ù–ò–ï –ò–¢–û–ì–û–í–û–ì–û –û–¢–ß–ï–¢–ê")
    
    report_lines = []
    report_lines.append("="*70)
    report_lines.append("–û–¢–ß–ï–¢ –ü–û –ü–û–î–ì–û–¢–û–í–ö–ï –î–ê–ù–ù–´–• –î–õ–Ø BIGQUERY")
    report_lines.append("="*70)
    report_lines.append(f"\n–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"\n–°–æ–∑–¥–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(created_files)}")
    report_lines.append("\n" + "-"*70)
    report_lines.append("–°–ü–ò–°–û–ö –°–û–ó–î–ê–ù–ù–´–• –§–ê–ô–õ–û–í:")
    report_lines.append("-"*70)
    
    total_size = 0
    for filepath in created_files:
        filename = os.path.basename(filepath)
        file_size = os.path.getsize(filepath) / 1024  # KB
        total_size += file_size
        
        df = pd.read_csv(filepath)
        rows = len(df)
        cols = len(df.columns)
        
        report_lines.append(f"\n{filename}:")
        report_lines.append(f"  –°—Ç—Ä–æ–∫: {rows:,}")
        report_lines.append(f"  –°—Ç–æ–ª–±—Ü–æ–≤: {cols}")
        report_lines.append(f"  –†–∞–∑–º–µ—Ä: {file_size:.1f} KB")
    
    report_lines.append("\n" + "-"*70)
    report_lines.append(f"–ò–¢–û–ì–û: {total_size:.1f} KB ({total_size/1024:.2f} MB)")
    report_lines.append("="*70)
    
    report_text = "\n".join(report_lines)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    report_path = os.path.join(OUTPUT_DIR, 'data_preparation_report.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_text)
    
    print(report_text)
    print(f"\n‚úì –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")

# ============================================================================
# –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø
# ============================================================================

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
    
    print("\n" + "="*70)
    print("üöÄ –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø BIGQUERY")
    print("="*70)
    print(f"–í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)
    
    created_files = []
    
    try:
        # ===== –®–ê–ì 1: –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• =====
        df = load_data(INPUT_FILE)
        
        # ===== –®–ê–ì 2: –°–û–ó–î–ê–ù–ò–ï DEMOGRAPHICS =====
        demo_df = create_demographics_table(df)
        demo_file = save_table(demo_df, 'demographics.csv', OUTPUT_DIR)
        if demo_file:
            created_files.append(demo_file)
        
        # ===== –®–ê–ì 3: –°–û–ó–î–ê–ù–ò–ï –¢–ï–•–ù–û–õ–û–ì–ò–ß–ï–°–ö–ò–• –¢–ê–ë–õ–ò–¶ =====
        print_header("üîß –°–û–ó–î–ê–ù–ò–ï –¢–ï–•–ù–û–õ–û–ì–ò–ß–ï–°–ö–ò–• –¢–ê–ë–õ–ò–¶ (UNPIVOT)")
        
        for source_column, (tech_type, status) in TECH_COLUMNS_MAP.items():
            # –°–æ–∑–¥–∞–µ–º unpivot —Ç–∞–±–ª–∏—Ü—É
            tech_df = create_technology_unpivot_table(df, source_column, tech_type, status)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            if tech_df is not None:
                filename = f"{tech_type}_{status}.csv"
                tech_file = save_table(tech_df, filename, OUTPUT_DIR)
                if tech_file:
                    created_files.append(tech_file)
        
        # ===== –®–ê–ì 4: –í–ê–õ–ò–î–ê–¶–ò–Ø =====
        validate_data_integrity(df, created_files)
        
        # ===== –®–ê–ì 5: –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ =====
        create_summary_report(created_files)
        
        # ===== –ó–ê–í–ï–†–®–ï–ù–ò–ï =====
        print_header("‚úÖ –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
        print(f"\nüìÅ –í—Å–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUTPUT_DIR}/")
        print(f"üìä –°–æ–∑–¥–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(created_files)}")
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        print("\n" + "="*70)
        print("üìù –°–õ–ï–î–£–Æ–©–ò–ô –®–ê–ì:")
        print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python scripts/03_upload_to_bigquery.py")
        print("="*70)
        
        return 0
        
    except Exception as e:
        print_header("‚ùå –û–®–ò–ë–ö–ê!")
        print(f"\n{type(e).__name__}: {e}")
        
        import traceback
        print("\n–ü–æ–ª–Ω—ã–π traceback:")
        print(traceback.format_exc())
        
        return 1

# ============================================================================
# –¢–û–ß–ö–ê –í–•–û–î–ê
# ============================================================================

if __name__ == "__main__":
    exit(main())